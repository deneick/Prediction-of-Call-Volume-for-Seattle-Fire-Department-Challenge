{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of Call Volume for Seattle Fire Department"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dispatching emergency calls in a city is challenging: There are large variations over a year, saison\n",
    "and especially time of day. Someone has to decide how many dispatchers are on duty for every day.\n",
    "Hence, we are looking for a model, which can predict the call volume for each day of the year and\n",
    "each hour of the day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function, that writes the code in .py files\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "        print(\"Appended to file \", file)\n",
    "    else:\n",
    "        print('Written to file:', file)\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell)        \n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run preprocessing.py\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sodapy import Socrata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we get the data from 'data.seattle.gov' using the Open Data API Socrata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a preprocessing.py\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    '''\n",
    "        Gets the data from data.seattle.gov past 2015\n",
    "    '''\n",
    "    client = Socrata(\"data.seattle.gov\", None)\n",
    "\n",
    "    results = client.get(\"kzjm-xkqj\", select='Datetime', limit=int(1e7),\n",
    "                         where=\"date_extract_y(datetime) >= 2015\")\n",
    "\n",
    "    df = pd.DataFrame.from_records(results)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we preprocess the data, to get Features, that we can use in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a preprocessing.py\n",
    "\n",
    "\n",
    "def get_features(df, freq='h', cutoff=True):\n",
    "    '''\n",
    "        Produces features out of the raw data\n",
    "        Parameters:\n",
    "            df: raw data\n",
    "            freq: frequency in which we want the data to be\n",
    "                  ('h': hours, 'd': days)\n",
    "            cutoff: whether to cut off the last datapoint\n",
    "    '''\n",
    "    # Convert Datetime column into datetime format\n",
    "    df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
    "\n",
    "    # Create Count column, which will contain the Call Volumes\n",
    "    df['Count'] = df.index\n",
    "\n",
    "    # Group the data and count the entries per frequency\n",
    "    df = df.groupby(pd.Grouper(key='Datetime', freq=freq)).count()\n",
    "\n",
    "    # We cut off the last datapoint, since it may be incomplete\n",
    "    if cutoff:\n",
    "        df = df[:-1]\n",
    "\n",
    "    # Now, we create the actual features\n",
    "    df['Year'] = df.index.year\n",
    "    df['Month'] = df.index.month\n",
    "    df['Day'] = df.index.day\n",
    "    if freq == 'h':\n",
    "        df['Hour'] = df.index.hour\n",
    "    df['DayOfYear'] = df.index.dayofyear\n",
    "    df['DayOfWeek'] = df.index.dayofweek\n",
    "\n",
    "    # Additional feature, that contains, if there is currently\n",
    "    # a Covid Emergency in Seattle\n",
    "    start = datetime.strptime(\"01-03-2020\", \"%d-%m-%Y\")\n",
    "    end = datetime.strptime(\"01-03-2021\", \"%d-%m-%Y\")\n",
    "    df['Covid'] = ((start <= df.index) & (df.index <= end)).astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a preprocessing.py\n",
    "\n",
    "\n",
    "def get_XY(df):\n",
    "    '''\n",
    "        Splits the data into features and labels\n",
    "    '''\n",
    "    X = df.drop('Count', axis=1)\n",
    "    Y = df['Count']\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run train.py\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from preprocessing import get_data, get_features, get_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a Randomized Grid Search to find the best parameters for the estimator. As an estimator, we use a XGBoost Regressor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a train.py\n",
    "\n",
    "\n",
    "def get_best_regressor(trainX, trainY):\n",
    "    '''\n",
    "        Compute the best XGBoost regressor using a Randomized Grid Search\n",
    "        Parameters:\n",
    "            trainX: training features\n",
    "            trainY: training labels\n",
    "    '''\n",
    "    RSEED = 42\n",
    "    # Hyperparameter grid\n",
    "    param_grid = {'n_estimators': np.linspace(100, 1000).astype(int),\n",
    "                  'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "                  'max_leaves': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "                  'learning_rate': [0.001, 0.01, 0.1, 1],\n",
    "                  'subsample': [0.8, 0.85, 0.9, 0.95, 1]\n",
    "                  }\n",
    "\n",
    "    # Estimator for use in random search\n",
    "    estimator = XGBRegressor(random_state=RSEED)\n",
    "\n",
    "    # Create the random search model\n",
    "    rs = RandomizedSearchCV(estimator, param_grid, n_jobs=-1, cv=3,\n",
    "                            n_iter=10, verbose=5, random_state=RSEED)\n",
    "\n",
    "    # Fit\n",
    "    rs.fit(trainX, trainY)\n",
    "\n",
    "    print('best parameters for estimator: ', rs.best_params_)\n",
    "\n",
    "    return rs.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to run the functions to get the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a train.py\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get the Data\n",
    "    data = get_data()\n",
    "    # Get the Features\n",
    "    df = get_features(data)\n",
    "    X, Y = get_XY(df)\n",
    "    # Get the model\n",
    "    model = get_best_regressor(X, Y)\n",
    "    # Save the model\n",
    "    model.save_model('model.json')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run predict.py\n",
    "import pandas as pd\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import sys\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from preprocessing import get_features, get_XY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions for the future, we need some input for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a predict.py\n",
    "\n",
    "\n",
    "def get_testX(start_date, date_range=30):\n",
    "    '''\n",
    "        Produces forecast input for the model.\n",
    "        Parameters:\n",
    "            start_date: start_date of the test data\n",
    "            date_range: number of days to forecast\n",
    "    '''\n",
    "    test_range = pd.date_range(start_date, periods=date_range*24, freq='h')\n",
    "    df_test_range = pd.DataFrame(test_range, columns=['Datetime'])\n",
    "    df_test = get_features(df_test_range, cutoff=False)\n",
    "    X, Y = get_XY(df_test)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the model and make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a predict.py\n",
    "\n",
    "\n",
    "def main(start_date, date_range):\n",
    "    # Load the model\n",
    "    model = XGBRegressor()\n",
    "    model.load_model('model.json')\n",
    "\n",
    "    # Get the input\n",
    "    testX = get_testX(start_date=start_date, date_range=date_range)\n",
    "    # Make predictions\n",
    "    pred = model.predict(testX)\n",
    "\n",
    "    # Print Prediction\n",
    "    print('Prediction for', testX.index[0], ': ', pred[0])\n",
    "\n",
    "    # Save predictions\n",
    "    testX['pred'] = pred\n",
    "    df_test = testX['pred']\n",
    "    df_test.to_csv('results.csv')\n",
    "    print(date_range, 'days of predictions saved in results.csv')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Try to load the date from the system args\n",
    "    try:\n",
    "        start_date = datetime.strptime(str(sys.argv[1]), \"%d-%m-%Y:%H\")\n",
    "    except:\n",
    "        print('invalid date, will use now')\n",
    "        start_date = datetime.now()\n",
    "        start_date = start_date.replace(minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    # Try to load the forecast range from the system args\n",
    "    try:\n",
    "        date_range = int(sys.argv[2])\n",
    "    except:\n",
    "        print('invalid number, will forecast one day')\n",
    "        date_range = 1\n",
    "    main(start_date, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model, we want to train the model on a smaller subset and test it on an unseen subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run test.py\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "from preprocessing import get_data, get_features, get_XY\n",
    "from train import get_best_regressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to make the train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a test.py\n",
    "\n",
    "\n",
    "def get_train_test_split(df, split):\n",
    "    '''\n",
    "        Splits the data in train and test subsets.\n",
    "        Parameters:\n",
    "            df: data to split\n",
    "            split: Datetime where the data is split\n",
    "    '''\n",
    "    dftrain = df[df.index < split]\n",
    "    dftest = df[df.index >= split]\n",
    "\n",
    "    trainX, trainY = get_XY(dftrain)\n",
    "    testX, testY = get_XY(dftest)\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we actually run the functions to get predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%write_and_run -a test.py\n",
    "\n",
    "\n",
    "# Get the Data\n",
    "data = get_data()\n",
    "# Get the Features\n",
    "df = get_features(data)\n",
    "# Get the Train Test Split\n",
    "trainX, trainY, testX, testY = get_train_test_split(df, split=datetime.strptime(\"01-08-2022\", \"%d-%m-%Y\"))\n",
    "\n",
    "# Get the model\n",
    "model = get_best_regressor(trainX, trainY)\n",
    "\n",
    "# Make predictions\n",
    "pred_test = model.predict(testX)\n",
    "pred_train = model.predict(trainX)\n",
    "\n",
    "# Calculate Errors\n",
    "#print('RSME on training set: ', mean_squared_error(trainY, pred_train, squared=False))\n",
    "print('MAE on training set: ', mean_absolute_error(trainY, pred_train))\n",
    "print('R^2 on training set: ', r2_score(trainY, pred_train))\n",
    "#print('RSME on test set: ', mean_squared_error(testY, pred_test, squared=False))\n",
    "print('MAE on test set: ', mean_absolute_error(testY, pred_test))\n",
    "print('R^2 on test set: ', r2_score(testY, pred_test))\n",
    "\n",
    "# Save predictions\n",
    "df['pred'] = np.concatenate([pred_train, pred_test])\n",
    "df = df[['pred', 'Count']]\n",
    "df.to_csv('predictions_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have predictions. We already have the MAE and the R^2 values. The MAE tells us, how far the prediction is in average away from the actual Call Volume. This seems quite good. The R^2 value on the test set is quite low, this means that the model does not account very much of the variance. This is fine, since we have a lot of variance in the data. So, let's have a closer look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "def plot_results(df, split, offset1, offset2=-1):\n",
    "    '''\n",
    "        Plots the results\n",
    "        Parameters:\n",
    "            df: data to plot\n",
    "            split: length of the training samples\n",
    "            offset1: offset for the left border \n",
    "                     (negative values will show train data)\n",
    "            offset2: offset for the right border\n",
    "                     (-1 will show all values)\n",
    "    '''\n",
    "    if offset2 == -1:\n",
    "        offset2 -= split\n",
    "    sns.lineplot(df[split+offset1:split+offset2], x=df.index[split+offset1:split+offset2], y='Count', label='Call Volume')\n",
    "    sns.lineplot(df[split+offset1:split+offset2], x=df.index[split+offset1:split+offset2], y='pred', label='Prediction')\n",
    "    if offset1 < 0:\n",
    "        plt.axvline(x=df.index[split], color='r')\n",
    "    plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next plot, we see the prediction for the full Month August. The red line seperates the training set from the testing set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df, len(trainX), -100, 31*24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a closer look. The next plot shows the prediction only for the first week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df, len(trainX), -24*2, 24*7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the Prediction fits the Call Volumes pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Call Volumes per day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to forecast further into the future, it may make more sense to look at the daily Call Volumes instead of the hourly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_days = get_features(data, freq='d')\n",
    "trainX, trainY, testX, testY = get_train_test_split(df_days, split = datetime.strptime(\"01-09-2021\", \"%d-%m-%Y\"))\n",
    "model = get_best_regressor(trainX, trainY)\n",
    "\n",
    "pred_test = model.predict(testX)\n",
    "pred_train = model.predict(trainX)\n",
    "\n",
    "print('MAE on test set: ', mean_absolute_error(testY, pred_test))\n",
    "print('R^2 on test set: ', r2_score(testY, pred_test))\n",
    "\n",
    "df_days['pred'] = np.concatenate([pred_train, pred_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next plot, we can see the forecast for one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(df_days, len(trainX), -1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, that the model has some difficulties in forecasting the daily Call Volumes accurately in the more farer future. Daily Call Volumes seem to be more random and in general it is just more difficult to forecast such a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we built a model that can accurately forecast the hourly Call Volumes in the next weeks to months. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forecasting Call Volumes in the far future, like one year, is quite difficult since there are more factors that play a role. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea48eaabdc8bde486507545d6bc00cf10dc5fb6ef8ce20334829a8ad66f30436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
